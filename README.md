# FLiD: **F**ramework for **L**abel-L**i**mited **D**ynamic Node Classification
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?logo=PyTorch&logoColor=white)](https://pytorch.org/)

This repository is built for the paper

**PTCL: Pseudo-Label Temporal Curriculum Learning for
Label-Limited Dynamic Graph**

FLiD is a novel framework for dynamic graph learning where only final timestamp labels are available. Designed for extensibility and fairness, it supports cutting-edge research in temporal graph analysis through:

<p align="center">
  <img src='images/overview.png'>
</p>

## ğŸš€ Key Features
- **Novel Architecture**
  * Dual-model EM optimization with pseudo-label enhancement
  * Support for below paradigms with modular components: 
    * CFT (Copy-Final Timestamp labels) * DLS (Dynamic Label Supervision)
    * NPL (Naive Pseudo-Labels) 
    * SEM (Standard EM) 
    * PTCL-2D (PTCL with 2 Decoders) 
    * PTCL-2D
  ![Methods](images/methods.png)
  * Support for below pseudo-labels enhancement methods:
    * Confidence Score Threshold (CST)
    * Entropy of Softmax Trajectory (EST) 
    * Temporal Curriculum Learning

  * Support for below various backbone models for Label-limited Dynamic Node Classification:
    * TGAT 
    * TGN 
    * GraphMixer
    * TCL
    * DyGFormer 

  * Support for below datasets:
    |                         | Wikipedia | Reddit  | Dsub   | CoOAG  |
    |-------------------------|-----------|---------|--------|--------|
    | **Nodes**               | 9,227     | 10,984  | 150,000| 9,559  |
    | **Edges**               | 157,474   | 672,447 | 168,154| 114,337|
    | **Duration**            | 1 month   | 1 month | 1 year | 22 years|
    | **Total classes**       | 2         | 2       | 2      | 5      |
  * Adaptive weight scheduling (`gt_weight` decay)

- **Comprehensive Support**
  ```python
  # Flexible training workflows
  for k in range(args.num_em_iters):
      e_step(...)  # Expectation step
      m_step(...)  # Maximization step
      update_pseudo_labels(...)  # Label refinement
  ```

- **Research-Ready Infrastructure**
  - Automatic metric tracking (AUC/Accuracy)
  - Early stopping with model checkpointing
  - Reproducible seed management
  - Multi-run experiment support

## ğŸ“¦ Installation
```bash
conda create -n flid python=3.8
conda activate flid
pip install -r requirements.txt
```

## ğŸ›  Usage

-  æ•°æ®é¢„å¤„ç†

    ä½¿ç”¨ `preprocess` å¤„ç†æ•°æ®ã€‚

- Step 1: Warmup

    1. é…ç½®ä»¥ä¸‹å‚æ•°è¿›è¡Œ warmup è®­ç»ƒï¼š
    - `model_name`ï¼šé€‰æ‹©æ¨¡å‹ï¼ˆå¦‚ `TGAT`ï¼‰
    - `gpu`ï¼šæŒ‡å®šä½¿ç”¨çš„ GPUï¼ˆå¦‚ `5`ï¼‰
    - `dataset`ï¼šé€‰æ‹©æ•°æ®é›†ï¼ˆå¦‚ `reddit` æˆ– `wikipedia`ï¼‰
    - `threshold`ï¼šä¼ªæ ‡ç­¾é˜ˆå€¼ï¼ˆå¦‚ `0.5`ï¼‰
    - `gt_weight`ï¼šä¼ªæ ‡ç­¾çš„æƒé‡ï¼ˆå¦‚ `0.9`ï¼‰

    è¿è¡Œ warmup é˜¶æ®µï¼š

    ```bash
    bash warmup.sh
    ```

- Step 2: è®­ç»ƒ

    1. é…ç½®ä»¥ä¸‹å‚æ•°è¿›è¡Œè®­ç»ƒï¼š
    - `method`ï¼šé€‰æ‹©è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚ `PTCL`, `SEM`, `NPL`ï¼‰
    - `dataset`ï¼šé€‰æ‹©æ•°æ®é›†ï¼ˆå¦‚ `reddit`, `wikipedia`, `oag`ï¼‰
    - `gt_weight`ï¼šä¼ªæ ‡ç­¾çš„æƒé‡ï¼ˆå¦‚ `0.5`ï¼‰
    - `alphas`ï¼šè®¾ç½®ä¸åŒçš„è¶…å‚æ•°ï¼ˆå¦‚ `0.1`ï¼‰
    - `gpus`ï¼šæŒ‡å®šä½¿ç”¨çš„ GPUï¼ˆå¦‚ `[1]`ï¼‰
    - `max_tasks_per_gpu`ï¼šæ¯ä¸ª GPU æœ€å¤§ä»»åŠ¡æ•°ï¼ˆå¦‚ `1`ï¼‰

    è¿è¡Œè®­ç»ƒè„šæœ¬ï¼š

    ```bash
    bash train.sh
    ```

- ç»“æœ

    - è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ`logs/` ç›®å½•å°†ä¿å­˜è®­ç»ƒæ—¥å¿—ã€‚
    - è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ`results/` ç›®å½•å°†ä¿å­˜è®­ç»ƒç»“æœã€‚
    - å¯ä»¥æ ¹æ®è¾“å‡ºçš„ `AUC` å’Œ `ACC` è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

## ğŸ“Š Method Comparison

| Method       | EM Steps | Pseudo-Label Strategy     | Key Characteristics                 |
|--------------|----------|---------------------------|-------------------------------------|
| **CFT**      | None     | Copy Final Labels         | Baseline with label propagation     |
| **DLS**      | None     | Full Supervision          | With dynamic labels             |
| **NPL**      | None     | Joint Optimization with generated pseudo-labels       | Single-phase training               |
| **SEM**      | Full  | 2-stage generate pseudo-labels          | Standard EM Implementation          |
| **PTCL**     | Full     | E-step generate pseudo-labels       | Dual-phase EM + Temporal Filtering  |
| **PTCL-2D**  | Full     | Dual-Decoder Architecture | Prevents confirmation bias          |


## ğŸ“‚ Repository Structure
```
FLiD/
â”œâ”€â”€ models/                    # Various Backbone
â”œâ”€â”€ logs/                      # Training metrics & logs
â”‚   â””â”€â”€ {method}/              # Per-method organization
â”œâ”€â”€ results/                   # Trainin results
â”‚   â””â”€â”€ {method}/              # Per-method organization
â”œâ”€â”€ saved_models/              # Model checkpoints
â”œâ”€â”€ process_data/              # Preprocess tools
â”œâ”€â”€ processed_data/            # Preprocessed datasets
â”‚
â”œâ”€â”€ PTCL/                      # Core EM implementation
â”‚   â”œâ”€â”€ EM_init.py             # Model initialization
â”‚   â”œâ”€â”€ EM_warmup.py           # Model warmup
â”‚   â”œâ”€â”€ E_step.py              # Expectation phase
â”‚   â””â”€â”€ M_step.py              # Maximization phase
â”‚   â”œâ”€â”€ trainer.py             # Trainier initialization
â”‚   â”œâ”€â”€ utils.py               # Important utils tools
â”‚
â”œâ”€â”€ NPL/                       # Core NPL implementation
â”‚   â”œâ”€â”€ NPL_init.py            # Model initialization
â”‚   â”œâ”€â”€ NPL.py                 # Training phase
â”‚
â”œâ”€â”€ SEM/                       # Core NPL implementation
â”‚   â”œâ”€â”€ E_step.py              # Expectation phase
â”‚   â”œâ”€â”€ M_step.py              # Maximization phase
â”‚
â”œâ”€â”€ utils/                     # Infrastructure
â”‚   â”œâ”€â”€ DataLoader.py          # Dataset processing
â”‚   â”œâ”€â”€ EarlyStopping.py       # Training control
â”‚   â””â”€â”€ load_configs.py        # Logging setup
â”‚   â””â”€â”€ utils.py               # Useful tools
â”‚   â””â”€â”€ metrics.py             # Metrics calculating tools
â”‚
â””â”€â”€ train.py                   # Experiment entry point
```

## ğŸ“ˆ Evaluation Metrics
Framework tracks multiple metrics through `log_and_save_metrics()`:
```python
# Sample metric output
2023-11-15 14:30:00 - Estep - INFO - Test Metrics:
{
    "AUC": 0.892,
    "Accuracy": 0.814,
    "Loss": 0.423
}
```

## ğŸ¤ Contributing
We welcome contributions! Please follow our [contribution guidelines](CONTRIBUTING.md) and:
- Use consistent logging practices
- Maintain backward compatibility
- Add unit tests for new features

## ğŸ“œ Citation
If using FLiD in your research, please cite:
```bibtex
@article{flid2023,
  title={PTCL: Pseudo-Label Temporal Curriculum Learning for Label-Limited Dynamic Graph},
  author={Shengtao Zhang, Haokai Zhang, Shiqi Lou, Zicheng Wang, Zinan Zeng, Yilin Wang, Minnan Luo},
  journal={Waiting},
  year={2025}
}
``` 

## License
This project is licensed under the [MIT License](LICENSE).